{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61585317",
   "metadata": {},
   "source": [
    "# Geometric View of Gaussians\n",
    "\n",
    "The Gaussian distribution is one of the most widely used distributions in machine learning, which we will see as these notes progress. For a single variable the Gaussian is defined as \n",
    "\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{\\frac{1}{2}}}e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma^2$ is the variance. The multivariate Gaussian is defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\pmb{x}|\\pmb{\\mu}, \\pmb{\\Sigma}) = \\frac{1}{(2\\pi)^{\\frac{D}{2}}}\\frac{1}{|\\pmb{\\Sigma}|^{\\frac{1}{2}}}e^{-\\frac{1}{2}(\\pmb{x} - \\pmb{\\mu})^T\\pmb{\\Sigma}^{-1}(\\pmb{x} - \\pmb{\\mu})}\n",
    "$$\n",
    "\n",
    "where $\\pmb{\\mu}$ is a D-dimensional mean vector, $\\pmb{\\Sigma}$ is a $D \\times D$ covariance matrix and $|\\pmb{\\Sigma}|$ is the determinant of the covariance. For ease of notation, I will drop the convention of using bold text to respresent vector values and instead use context to indicate when we are talking about a vector or scalar value.\n",
    "\n",
    "We can define the functional dependence of the multivariate Gaussian on x through the **Mahalanobis distance** $\\Delta$ between x and the mean:\n",
    "\n",
    "$$\n",
    "\\Delta^2 = (x - \\mu)^T \\Sigma^{-1}(x - \\mu)\n",
    "$$\n",
    "\n",
    "***Statement:*** The precision matrix $\\Sigma^{-1}$ can be taken to be symmetric without loss of generality.\n",
    "\n",
    "***Proof: (PRML Exercise 2.17)*** Let $A = \\Sigma^{-1}$ for notational purposes. We can express the precision matrix as a sum of its symmetric and antisymmetric parts. Explicitly:\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T)\n",
    "$$\n",
    "\n",
    "Then the exponent in the Gaussian definition is\n",
    "\n",
    "$$\n",
    "(x - \\mu)^TA(x - \\mu) = x^Tax - x^TA\\mu - \\mu^TAx + \\mu^TA\\mu\n",
    "$$\n",
    "\n",
    "The terms $x^TAx$ are called the **quadratic form** of the matrix $A$. We will essentially show the quadratic form of the antisymmetric part of a matrix is 0.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x^TAx &= x^T(\\frac{1}{2}(A + A^T) + \\frac{1}{2}(A - A^T))x \\\\\n",
    "&= \\frac{1}{2}[x^TAx + x^TA^Tx + x^TAx - x^TAx] \\\\\n",
    "&= \\frac{1}{2}(2x^TAx) \\\\\n",
    "&= x^TAx\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we see the only part that survives is the symmetric part of the matrix. This holds for the other terms as well. Thus we can treat the precision matrix as symmetric without loss of generality. $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e80fe",
   "metadata": {},
   "source": [
    "Consider the eigenvector equation for the covariance matrix\n",
    "\n",
    "$$\n",
    "\\Sigma u_i = \\lambda_i u_i\n",
    "$$\n",
    "\n",
    "for $i = \\{1, \\dots, D\\}$. Using the results from the appendix, we know that because $\\Sigma$ is real and symmetric, its eigenvalues are real and the eigenvectors for an orthonormal set\n",
    "\n",
    "$$\n",
    "u_i^Tu_j = I_{ij}\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix.\n",
    "\n",
    "Note also that since $\\Sigma$ is real and symmetric, it is orthogonally diagonalizable with the diagonal entries being the eigenvalues of $\\Sigma$. Thus we have $\\Sigma = UDU^T$ for $U,D \\in \\mathbb{R}^D$ and apply it into some arbitrary $x \\in \\mathbb{R}^D$. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    UDU^Tx &= UD \\begin{bmatrix}u_1^Tx \\\\ \\vdots \\\\ u_D^Tx\\end{bmatrix} \\\\\n",
    "           &= U \\begin{bmatrix}\\lambda_1 u_1^Tx \\\\ \\vdots \\\\ \\lambda_D u_D^Tx\\end{bmatrix} \\\\\n",
    "           &= \\sum_{i=1}^D \\lambda_i u_i u_i^T x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this is the same as the above eigenvector equation, since in the case that $x = u_i$, $\\lambda_i u_i u_i^T u_i = \\lambda_i u_i$ by orthogonality. \n",
    "\n",
    "Similarly, the inverse covariance matrix can be expressed as \n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma^{-1} &= (UDU^T)^{-1} \\\\\n",
    "            &= (U^T)^{-1}D^{-1}U^{-1} \\\\\n",
    "            &= U^T D^{-1} U\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "by definition of orthogonality. Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    UDU^Tx &= UD \\begin{bmatrix}u_1^Tx \\\\ \\vdots \\\\ u_D^Tx\\end{bmatrix} \\\\\n",
    "           &= U \\begin{bmatrix}\\lambda_1^{-1} u_1^Tx \\\\ \\vdots \\\\ \\lambda_D^{-1} u_D^Tx\\end{bmatrix} \\\\\n",
    "           &= \\sum_{i=1}^D \\lambda_i^{-1} u_i u_i^T x\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Plugging this into the quadratic form gives\n",
    "\n",
    "$$\n",
    "\\Delta^2 = \\sum_{i=1}^D\\frac{y_i^2}{\\lambda_i}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "y_i = u_i^T(x - \\mu)\n",
    "$$\n",
    "\n",
    "which can be interpreted as a coordinate system with bases $u_i$, center $\\mu$ and scaling factors $\\lambda_i^{\\frac{1}{2}}$. This gives us a geometric understanding of the Gaussian distribution. \n",
    "\n",
    "Lastly, in the appendix we see that $|J| = 1$ and $|\\Sigma|$ is just the sum of its eigenvalues along the diagonal of its orthogonal decomposition. Thus the Gaussian in this geometric coordinate system is:\n",
    "\n",
    "$$\n",
    "p(y) = \\prod_{j=1}^D \\frac{1}{(2\\pi \\lambda_j)^{\\frac{1}{2}}}e^{-\\frac{y_j^2}{2 \\lambda_j}}\n",
    "$$\n",
    "\n",
    "This representation can be useful since it essentially breaks down the multivariate Gaussian into a product of D independent univariate Gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3095090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Plot 2D Gaussian with axes to show geometry (let lambda and mu be a parameter). \n",
    "#Also plot aligned and isotropic covariance case to show limitations a la page 84. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec1816",
   "metadata": {},
   "source": [
    "# Understanding $\\mu$ and $\\Sigma$\n",
    "\n",
    "We quickly wish to show the interpretation of $\\mu$ and $\\Sigma$ in the multivariate Gaussian as the mean and covariance respectively. \n",
    "\n",
    "We define the **moment generating function (mgf)** for some n-dimensional random vector $X \\in \\mathbb{R}^n$ as\n",
    "\n",
    "$$\n",
    "M_X(t) := E[e^{t^TX}] \n",
    "$$\n",
    "\n",
    "For a multivariate Gaussian, the mgf is given to be (derivation in appendix):\n",
    "\n",
    "$$\n",
    "M_X(t) = exp(t^T\\mu + \\frac{1}{2}t^T\\Sigma t)\n",
    "$$\n",
    "\n",
    "Thus we can find the first and second moments with \n",
    "$$\n",
    "\\frac{dM_X(t)}{dt}\\vert_{t=0}\n",
    "$$.\n",
    "\n",
    "**TODO: FINISH THIS SECTION!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb80de",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "*Statement: (PRML Exercise 1.35)* The entropy of a univariate Gaussian \n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{1}{2}}}e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "is given by\n",
    "\n",
    "$$\n",
    "H[x] = \\frac{1}{2}(1 + ln(2\\pi\\sigma^2))\n",
    "$$\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 2.13)* KL Divergence of two multivariate Gaussians $p(x) = \\mathcal{N}(x|\\mu, \\Sigma)$ and $q(x) = \\mathcal{N}(x|m, L)$.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 2.14)* The multivariate distribution with maximum entropy, for a given covariance, is a multivariate Gaussian distribution.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 2.15)* The entropy of a multivariate Gaussian $\\mathcal{N}(x|\\mu, \\Sigma)$ is given by\n",
    "\n",
    "$$\n",
    "H[x] = \\frac{1}{2}ln|\\Sigma| + \\frac{D}{2}(1 + ln(2\\pi))\n",
    "$$\n",
    "\n",
    "where D is the dimensionality of x. \n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 2.18)* Given a real symmetric matrix $A$ with eigenvalue equation $Au_i = \\lambda_i u_i$, the eigenvalues $\\lambda_i$ are real and the set of eigenvectors satisfying this eigenvalue equation are orthonormal.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement:* Every real, symmetric matrix is orthogonally diagonalizable. \n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement:* The determinant of the Jacobian matrix going from the coordinate system $x_i$ to $y_i$ for a Gaussian is 1, and the determinant of the covariance matrix is given by $|\\Sigma| = \\sum_{i=1}^D \\lambda_i$.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement:* The moment generating function of a multivariate normal distribution is given by \n",
    "$$\n",
    "M_X(t) = exp(t^T\\mu + \\frac{1}{2}t^T\\Sigma t)\n",
    "$$\n",
    "\n",
    "*Proof:* **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef83b5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aeee10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
