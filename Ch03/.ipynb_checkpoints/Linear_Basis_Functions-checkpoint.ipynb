{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59446ef7-8111-4ee7-ac2a-a485577c38a4",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "The simplest linear model for regression is called **linear regression** (called so because it is linear w.r.t. parameters), which is given by\n",
    "\n",
    "$$\n",
    "y(x, w) = w_0 + w_1x_1 + \\dots + w_Dx_D\n",
    "$$\n",
    "\n",
    "where $x = (x_1, \\dots, x_D)^T$. However, the expressivity of this model is very low, since iti s linear w.r.t. the input variables. This can be extended by considering **basis functions** $\\phi_j(x)$ where\n",
    "\n",
    "$$\n",
    "y(x, w) = w_0 + \\sum_{j=1}^{M-1}w_j\\phi_j(x)\n",
    "$$\n",
    "\n",
    "The choice of basis function depends on the problem. For example, the \"Gaussian\" basis function can be given by\n",
    "\n",
    "$$\n",
    "\\phi_j(x) = exp\\{-\\frac{(x - \\mu_j)^2}{2s^2}\\}\n",
    "$$\n",
    "\n",
    "or a sigmoid basis function given by \n",
    "\n",
    "$$\n",
    "\\phi_j(x) = \\sigma(\\frac{x - \\mu_j}{s})\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1 + exp(-a)}\n",
    "$$\n",
    "\n",
    "These can be seen in the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee9dec0-6c8b-4a7d-8ef7-bd99a8a8bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Plot sigmoid and Gaussian and tanh basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8276c8-f817-47dc-a674-b3daecc3f9e0",
   "metadata": {},
   "source": [
    "# Maximum Likelihood and Least Squares\n",
    "\n",
    "Let $t$ be a target variable determined by a function $y(x, w)$ with additive Gaussian noise ($\\epsilon ~ \\mathcal{N}(0, \\beta^{-1})$) such that\n",
    "\n",
    "$$\n",
    "t = y(x, w) + \\epsilon\n",
    "$$\n",
    "\n",
    "then the probability of the target variable can be given by\n",
    "\n",
    "$$\n",
    "p(t|x, w, \\beta) = \\mathcal{N}(t|y(x, w), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "Thus the optimal solution will be the conditional mean of the target variable\n",
    "\n",
    "$$\n",
    "E[t|x] = \\int tp(t|x)dt = y(x, w)\n",
    "$$\n",
    "\n",
    "Note that this implies that distribution of t given x is unimodal. Consider some i.i.d. data $X = \\{x_1, \\dots, x_N\\}$ with corresponding $\\{t_N\\}$. Then the likelihood is given by\n",
    "\n",
    "$$\n",
    "p(t|X, w, \\beta) = \\prod_{n = 1}^N \\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1})\n",
    "$$\n",
    "\n",
    "We wish to maximize the log likelihood to find the MLE for $w_{ML}$. First taking the log of the likelihood above gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "lnp(t|w, \\beta) &= \\sum_{n=1}^N ln(\\mathcal{N}(t_n | w^T \\phi(x_n), \\beta^{-1}) \\\\\n",
    "&= \\sum_{n=1}^N ln(\\frac{1}{(2\\pi\\beta^{-1})^{\\frac{1}{2}}}) + \\sum_{n=1}^N ln(e^{-\\frac{1}{2\\beta^{-1}}(t - w^T\\phi(x_n))}) \\\\\n",
    "&= \\sum_{n=1}^N ln(1) - ln((2 \\pi \\beta^{-1})^{\\frac{1}{2}}) - \\sum_{n=1}^N \\frac{1}{2 \\beta^{-1}}(t - w^T \\phi(x_n)) \\\\\n",
    "&= \\sum_{n=1}^N ln(1) - \\sum_{n=1}^N ln((2 \\pi \\beta^{-1})^{\\frac{1}{2}}) - \\sum_{n=1}^N \\frac{1}{2 \\beta^{-1}}(t - w^T \\phi(x_n))\\\\\n",
    "&= \\frac{N}{2}ln(\\beta) - \\frac{N}{2}ln(2\\pi) + \\beta E_D(w)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we defined $E_D(w)$ as the sum of squares error function \n",
    "\n",
    "$$\n",
    "E_D(w) = \\frac{1}{2} \\sum_{n=1}^N(t_n - w^T\\phi(x_n))^2\n",
    "$$\n",
    "\n",
    "Using the property $\\frac{\\partial}{A} (AB) = B^T$, we get the gradient\n",
    "\n",
    "$$\n",
    "\\nabla ln p(t| w, \\beta) = \\beta \\sum_{n=1}^N (t_n - w^T \\phi(x_n)) \\phi(x_n)^T\n",
    "$$\n",
    "\n",
    "and setting this equal to 0 and solving for $w$ gives the solution $w_{MLE} = (\\Phi^T \\Phi)^{-1}\\Phi^T t$, known as the **normal equations** for least squares. Here $\\Phi$ is the $N \\times M$ **design matrix**. We can define the **Moore-Penrose pseudo-inverse** of the matrix as $\\Phi^\\dagger = (\\Phi^T \\Phi)^{-1}\\Phi^T$. \n",
    "\n",
    "### Bias and Precision\n",
    "\n",
    "We can repeat the calculation above on the bias $w_0$ and precision $\\beta$ to get a better intuition of these parameters during regression as well. First consider the bias term. We can make the bias parameter explicit in the sum of squares error:\n",
    "\n",
    "$$\n",
    "E_D(w) = \\frac{1}{2}\\sum_{n=1}^N (t_n - w_0 - \\sum_{j=1}^{M-1}w_j \\phi_j(x_n))^2\n",
    "$$\n",
    "\n",
    "Then taking the derivative of the log likelihood above and setting equal to 0 gives:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^N(t_n - w_0 - \\sum_{j=1}^{M - 1} w_j \\phi_j(x_n)) = 0 \\\\\n",
    "\\sum_{n=1}^N w_0 = \\sum_{n=1}^N(t_n - \\sum_{j=1}^{M - 1} w_j \\phi_j(x_n)) \\\\\n",
    "N w_0 = \\sum_{n=1}^N(t_n - \\sum_{j=1}^{M - 1} w_j \\phi_j(x_n)) \\\\\n",
    "w_0 = \\bar{t} - \\sum_{j=1}^{M - 1} w_j \\bar{\\phi_j}(x_n)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where we define $\\bar{t}$ and $\\bar{\\phi_j}$ as the average of the corresponding values. From this we see that the bias $w_0$ compensates for the difference between averages in the targets and the weighted sum of averages of the bias functions. \n",
    "\n",
    "Next we can consider the precision parameter $\\beta$. Taking the derivative of the log likelihood w.r.t. precision and setting equal to 0 gives\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{N}{2\\beta} - \\frac{1}{2}\\sum_{n=1}^N(t_n - w^T\\phi(x_n))^2 = 0 \\\\\n",
    "\\frac{N}{2\\beta} = \\sum_{n=1}^N(t_n - w^T\\phi(x_n))^2 \\\\\n",
    "\\frac{1}{\\beta_{MLE}} = \\frac{1}{N}\\sum_{n=1}^N(t_n - w^T\\phi(x_n))^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So the precision parameter is just the average error of the regression function's output and the target values, which makes sense given the parameter's name. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f157d1-8957-4046-9307-574ba5172a4b",
   "metadata": {},
   "source": [
    "## Sequential Learning\n",
    "\n",
    "Note that we can rewrite least squares as a sequential learning problem by applying **stochastic gradient descent**. Let the iteration number be defined by $\\tau$. Then we can define the gradient descent algorithm update on $w$ with:\n",
    "\n",
    "$$\n",
    "w^{(\\tau + 1)} = w^{(\\tau)} - \\eta \\nabla E_n\n",
    "$$\n",
    "\n",
    "where n denotes the nth data point, and $\\eta$ is a learning rate. For the sum-of-squares error function, we get \n",
    "\n",
    "$$\n",
    "w^{(\\tau + 1)} = w^{\\tau} + \\eta(t_n - w^{(\\tau)T}\\phi_n)\\phi_n\n",
    "$$\n",
    "\n",
    "where $\\phi_n = \\phi(x_n)$. This is known as **least-mean-squares**. \n",
    "\n",
    "## Regularized Least Squares\n",
    "\n",
    "In general, the total error function that needs to be minimized is of the form\n",
    "\n",
    "$$\n",
    "E_D(w) + \\lambda E_W(w)\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization coefficient. An example regularizer can be **weight decay** (or **parameter shrinkage** in statistics). \n",
    "\n",
    "$$\n",
    "E_W(w) = \\frac{1}{2}w^Tw\n",
    "$$\n",
    "\n",
    "This encourages weights to tend to 0 unless supported by the data. This error function keeps the error function quadractic in w, so it still has a closed form least-squares solution. \n",
    "\n",
    "More general, we can consider the p-norm\n",
    "\n",
    "$$\n",
    "E_W(w) = \\frac{1}{2}\\sum_{j=1}^M|w_j|^p\n",
    "$$\n",
    "\n",
    "Decreasing $p$ causes the model parameters $w_j$ to become more sparse. Thus, a model can be trained on limited data without overfitting by tuning the value of the regularization coeffcient $\\lambda$ to make the model sparse enough.\n",
    "\n",
    "## Multiple Target Outputs\n",
    "\n",
    "Consider the case where we want to predict $K > 1$ target variables, which we denote with a vector t. We can then use the same basis functions to model all components of the target vector such that\n",
    "\n",
    "$$\n",
    "y(x, w) = W^T\\phi(x)\n",
    "$$\n",
    "\n",
    "where $y \\in \\mathcal{R}^K$, $W \\in \\mathcal{R}^{M \\times K}$, and $\\phi(x) \\in \\mathcal{R}^M$, with the bias term $\\phi_0(x) = 1$. We consider the conditional distribution of the target vector the be an isotropic Gaussian of form\n",
    "\n",
    "$$\n",
    "p(t|x, W, \\beta) = \\mathcal{N}(t|W^T\\phi(x), \\beta^{-1}I)\n",
    "$$\n",
    "\n",
    "then the log likelihood function is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "lnp(T|X, W, \\beta) &= \\sum_{n=1}^N ln \\mathcal{N}(t_n|W^T\\phi(x_n),\\beta^{-1}I) \\\\\n",
    "&= \\frac{NK}{2}ln(\\frac{\\beta}{2\\pi}) - \\frac{\\beta}{2}\\sum_{n=1}^N||t_n - W^T\\phi(x_n)||^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $T \\in \\mathcal{R}^{N \\times K}$ is a matrix made from combining each $t_n^T$ as a row of T. Maximizing w.r.t. $W$ then gives\n",
    "\n",
    "$$\n",
    "W_{MLE} = (\\Phi^T\\Phi)^{-1}\\Phi^T T\n",
    "$$\n",
    "\n",
    "Note that this means we only ever need to solve for the pseudo-inverse matrix $\\Phi^{\\dagger} = (\\Phi^T\\Phi)^{-1}\\Phi^T$ and share it between all target values. This also extends to general non-isotopic Gaussian noise -- this derivation is found in the appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d67db9-1af7-40b9-9959-8e24885bc86e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d167055-f319-4e8e-bd0e-d8558985c111",
   "metadata": {},
   "source": [
    "*Statement: (PRML Exercise 3.1)* The $tanh$ function is related to the logistic sigmoid by \n",
    "\n",
    "$$\n",
    "tanh(a) = 2\\sigma(2a) - 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f7e0c-0fbd-46a8-b258-3cf97e308b5d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to view proof</summary>\n",
    "\n",
    "*Proof:* Remember that the sigmoid and tanh functions are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}\\\\\n",
    "\\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus we see that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 \\sigma(2a) - 1 &= \\frac{2}{1 + e^{-2a}} - 1 \\\\\n",
    "&= \\frac{1 - e^{-2a}}{1 + e^{-2a}} \\frac{e^a}{e^a} \\\\\n",
    "&= \\frac{e^a - e^{-a}}{e^a + e^{-a}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we see that the equivalence holds. Now consider a general linear combination of tanh and sigmoid functions of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(x, u) = u_0 + \\sum_{j=1}^M u_j tanh(\\frac{x - \\mu_j}{2s}) \\\\\n",
    "y(x, w) = w_0 + \\sum_{j=1}^M w_j \\sigma(\\frac{x - \\mu_j}{s})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $u$ and $w$ are different parameters. Using the relation above we can express these parameters as a scalar transformation from one to the other. Consider the combination of tanh functions.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(x, u) &= u_0 + \\sum_{j=1}^M u_j tanh(\\frac{x - \\mu_j}{2s}) \\\\\n",
    "&= u_0 + \\sum_{j=1}^M u_j [2 \\sigma(\\frac{x - \\mu_j}{s}) - 1] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Comparing this to $y(x, w)$ we see\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j \\sigma &= u_j (2 \\sigma - 1) \\\\\n",
    "w_j = \\frac{2 \\sigma - 1}{\\sigma} u_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= \\frac{2, \\sigma - 1}{\\sigma} u_j,  \\, \\, \\, 1 \\leq j \\leq M \\\\\n",
    "w_0 &= u_0\n",
    "\\end{align}\n",
    "$$\n",
    "$\\blacksquare$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b45f503-50cc-43d9-aa09-14923fa38555",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "*Statement: (PRML Exercise 3.1)* The $tanh$ function is related to the logistic sigmoid by \n",
    "\n",
    "$$\n",
    "tanh(a) = 2\\sigma(2a) - 1\n",
    "$$\n",
    "\n",
    "<details>\n",
    "  <summary>Click to view proof</summary>\n",
    "*Proof:* Remember that the sigmoid and tanh functions are given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}\\\\\n",
    "\\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus we see that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "2 \\sigma(2a) - 1 &= \\frac{2}{1 + e^{-2a}} - 1 \\\\\n",
    "&= \\frac{1 - e^{-2a}}{1 + e^{-2a}} \\frac{e^a}{e^a} \\\\\n",
    "&= \\frac{e^a - e^{-a}}{e^a + e^{-a}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we see that the equivalence holds. Now consider a general linear combination of tanh and sigmoid functions of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(x, u) = u_0 + \\sum_{j=1}^M u_j tanh(\\frac{x - \\mu_j}{2s}) \\\\\n",
    "y(x, w) = w_0 + \\sum_{j=1}^M w_j \\sigma(\\frac{x - \\mu_j}{s})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $u$ and $w$ are different parameters. Using the relation above we can express these parameters as a scalar transformation from one to the other. Consider the combination of tanh functions.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y(x, u) &= u_0 + \\sum_{j=1}^M u_j tanh(\\frac{x - \\mu_j}{2s}) \\\\\n",
    "&= u_0 + \\sum_{j=1}^M u_j [2 \\sigma(\\frac{x - \\mu_j}{s}) - 1] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Comparing this to $y(x, w)$ we see\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j \\sigma &= u_j (2 \\sigma - 1) \\\\\n",
    "w_j = \\frac{2 \\sigma - 1}{\\sigma} u_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= \\frac{2, \\sigma - 1}{\\sigma} u_j,  \\, \\, \\, 1 \\leq j \\leq M \\\\\n",
    "w_0 &= u_0\n",
    "\\end{align}\n",
    "$$\n",
    "$\\blacksquare$\n",
    "</details>\n",
    "\n",
    "*Statement: (PRML Exericise 1.25)* For a squared loss function of multiple target variables $t$, given by\n",
    "\n",
    "$$\n",
    "E[L(t, y(x)) = \\int \\int ||y(x) - t||^2 p(x, t)dxdt\n",
    "$$\n",
    "\n",
    "the function for which this loss is minimized is the conditional mean $y(x) = E_t[t|x]$. This reduces to the single target variable equation for a single target t.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 3.2)* The matrix $\\Phi(\\Phi^T\\Phi)^{-1}\\Phi^T$ takes any vector v and projects it onto the space spanned by the columns of $\\Phi$. \n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 3.5)* The minimization of the regularized error function \n",
    "\n",
    "$$\n",
    "E_W(w) = \\frac{1}{2}\\sum_{j=1}^M|w_j|^p\n",
    "$$\n",
    "\n",
    "is equivalent to minimizing the unregularized sum-of-squares error\n",
    "\n",
    "$$\n",
    "E_D(w) = \\frac{1}{2}\\sum_{n=1}^N(t_n - w^T\\phi(x_n))^2\n",
    "$$\n",
    "\n",
    "subject to the constraint\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^M|w_j|^p \\leq \\eta\n",
    "$$\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement: (PRML Exercise 3.6)* Consider a linear basis function regression model with a multivariate target variable t having a Gaussian distribution\n",
    "\n",
    "$$\n",
    "p(t|W, \\Sigma) = \\mathcal{N}(t| y(x, W), \\Sigma)\n",
    "$$\n",
    "\n",
    "where $y(x, w) = W^T\\phi(x)$. The maximum likelihood solution $W_{MLE}$ has the property that each column is given by an expression of the form \n",
    "\n",
    "$$\n",
    "W_{MLE} = (\\Phi^T\\Phi)^{-1}\\Phi^Tt\n",
    "$$\n",
    "\n",
    "which is the solution for an isotropic noise distribution. Also the maximum likelihood solution for the general covariance matrix $\\Sigma$ is given by\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{N}\\sum_{n=1}^N(t_n - W_{MLE}^T \\phi(x_n))(t_n - W_{MLE}^T \\phi(x_n))^T\n",
    "$$\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "*Statement:* Based on the solution $w_{MLE}$ above, we see that if two or more basis vectors $\\varphi_j$ (columns of the design matrix $\\Phi$), are co-linear, and thus close to degenerate, the matrix $\\Phi^T\\Phi$ is close to singular. This means the computation $(\\Phi^T\\Phi)^{-1}$ is very expensive. According to Section 3.1.2, the addition of a regularization term ensures that $\\Phi^T\\Phi$ is non-singular, even in the presence of degeneracies.\n",
    "\n",
    "*Proof:* **TODO**\n",
    "\n",
    "**TODO:** include reference to Moore-Penrose pseudo-inverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2841a9b5-a971-42bf-b27c-02bc91f00851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
